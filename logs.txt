
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ COMMAND ‚îÇ             ARGS              ‚îÇ PROFILE  ‚îÇ  USER  ‚îÇ VERSION ‚îÇ     START TIME      ‚îÇ      END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ start   ‚îÇ --driver=docker               ‚îÇ minikube ‚îÇ lemonu ‚îÇ v1.37.0 ‚îÇ 16 Feb 26 19:38 EST ‚îÇ 16 Feb 26 19:38 EST ‚îÇ
‚îÇ service ‚îÇ devops-kubernetes-api-service ‚îÇ minikube ‚îÇ lemonu ‚îÇ v1.37.0 ‚îÇ 16 Feb 26 19:44 EST ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2026/02/16 19:38:28
Running on machine: ashrees
Binary: Built with gc go1.24.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0216 19:38:28.789117  246789 out.go:360] Setting OutFile to fd 1 ...
I0216 19:38:28.789163  246789 out.go:413] isatty.IsTerminal(1) = true
I0216 19:38:28.789164  246789 out.go:374] Setting ErrFile to fd 2...
I0216 19:38:28.789166  246789 out.go:413] isatty.IsTerminal(2) = true
I0216 19:38:28.789233  246789 root.go:338] Updating PATH: /home/lemonu/.minikube/bin
W0216 19:38:28.789282  246789 root.go:314] Error reading config file at /home/lemonu/.minikube/config/config.json: open /home/lemonu/.minikube/config/config.json: no such file or directory
I0216 19:38:28.789477  246789 out.go:368] Setting JSON to false
I0216 19:38:28.790419  246789 start.go:130] hostinfo: {"hostname":"ashrees","uptime":30355,"bootTime":1771258354,"procs":418,"os":"linux","platform":"arch","platformFamily":"arch","platformVersion":"","kernelVersion":"6.18.7-arch1-1","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"64c538e4-65b8-4864-832e-e1a5e54d613c"}
I0216 19:38:28.790452  246789 start.go:140] virtualization: kvm host
I0216 19:38:28.799494  246789 out.go:179] üòÑ  minikube v1.37.0 on Arch 
W0216 19:38:28.802215  246789 preload.go:293] Failed to list preload files: open /home/lemonu/.minikube/cache/preloaded-tarball: no such file or directory
I0216 19:38:28.802258  246789 notify.go:220] Checking for updates...
I0216 19:38:28.802283  246789 driver.go:421] Setting default libvirt URI to qemu:///system
I0216 19:38:28.809475  246789 docker.go:123] docker version: linux-29.2.1:
I0216 19:38:28.809515  246789 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0216 19:38:28.832338  246789 info.go:266] docker info: {ID:3086d6b6-6b8e-4008-b5fa-72e751a01086 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem btrfs] [Supports d_type true] [Using metacopy true] [Native Overlay Diff false] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:24 OomKillDisable:false NGoroutines:45 SystemTime:2026-02-16 19:38:28.828650645 -0500 EST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:1 KernelVersion:6.18.7-arch1-1 OperatingSystem:Arch Linux OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:32708136960 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:ashrees Labels:[] ExperimentalBuild:false ServerVersion:29.2.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:dea7da592f5d1d2b7755e3a161be07f43fad8f75.m Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:0.31.0] map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:5.0.2]] Warnings:<nil>}}
I0216 19:38:28.832374  246789 docker.go:318] overlay module found
I0216 19:38:28.837983  246789 out.go:179] ‚ú®  Using the docker driver based on user configuration
I0216 19:38:28.840724  246789 start.go:304] selected driver: docker
I0216 19:38:28.840728  246789 start.go:918] validating driver "docker" against <nil>
I0216 19:38:28.840733  246789 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0216 19:38:28.840795  246789 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0216 19:38:28.863394  246789 info.go:266] docker info: {ID:3086d6b6-6b8e-4008-b5fa-72e751a01086 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem btrfs] [Supports d_type true] [Using metacopy true] [Native Overlay Diff false] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:24 OomKillDisable:false NGoroutines:45 SystemTime:2026-02-16 19:38:28.859812502 -0500 EST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:1 KernelVersion:6.18.7-arch1-1 OperatingSystem:Arch Linux OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:32708136960 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:ashrees Labels:[] ExperimentalBuild:false ServerVersion:29.2.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:dea7da592f5d1d2b7755e3a161be07f43fad8f75.m Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:0.31.0] map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:5.0.2]] Warnings:<nil>}}
I0216 19:38:28.863457  246789 start_flags.go:327] no existing cluster config was found, will generate one from the flags 
I0216 19:38:28.864132  246789 start_flags.go:410] Using suggested 7700MB memory alloc based on sys=31192MB, container=31192MB
I0216 19:38:28.864190  246789 start_flags.go:974] Wait components to verify : map[apiserver:true system_pods:true]
I0216 19:38:28.867117  246789 out.go:179] üìå  Using Docker driver with root privileges
I0216 19:38:28.869879  246789 cni.go:84] Creating CNI manager for ""
I0216 19:38:28.869897  246789 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0216 19:38:28.869900  246789 start_flags.go:336] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0216 19:38:28.869926  246789 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:7700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0216 19:38:28.872724  246789 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0216 19:38:28.875631  246789 cache.go:123] Beginning downloading kic base image for docker with docker
I0216 19:38:28.880774  246789 out.go:179] üöú  Pulling base image v0.0.48 ...
I0216 19:38:28.883461  246789 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0216 19:38:28.883528  246789 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I0216 19:38:28.899496  246789 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon, skipping pull
I0216 19:38:28.899501  246789 cache.go:147] gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in daemon, skipping load
I0216 19:38:28.936683  246789 lock.go:35] WriteFile acquiring /home/lemonu/.minikube/last_update_check: {Name:mka3940b9a6dfc873e54a2f2ee80c6aa028fb7a4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0216 19:38:28.939497  246789 out.go:179] üéâ  minikube 1.38.0 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.38.0
I0216 19:38:28.944849  246789 out.go:179] üí°  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0216 19:38:29.058083  246789 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I0216 19:38:29.058091  246789 cache.go:58] Caching tarball of preloaded images
I0216 19:38:29.058141  246789 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0216 19:38:29.063525  246789 out.go:179] üíæ  Downloading Kubernetes v1.34.0 preload ...
I0216 19:38:29.066227  246789 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I0216 19:38:29.262617  246789 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4?checksum=md5:994a4de1464928e89c992dfd0a962e35 -> /home/lemonu/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I0216 19:38:34.387643  246789 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I0216 19:38:34.387690  246789 preload.go:254] verifying checksum of /home/lemonu/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I0216 19:38:34.721018  246789 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I0216 19:38:34.721157  246789 profile.go:143] Saving config to /home/lemonu/.minikube/profiles/minikube/config.json ...
I0216 19:38:34.721167  246789 lock.go:35] WriteFile acquiring /home/lemonu/.minikube/profiles/minikube/config.json: {Name:mk7f9a4398df2b704356add382b9dae7f96cb9c0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0216 19:38:34.721234  246789 cache.go:232] Successfully downloaded all kic artifacts
I0216 19:38:34.721243  246789 start.go:360] acquireMachinesLock for minikube: {Name:mk83cf12d22ad3e4e698817e4288ddfed865899c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0216 19:38:34.721253  246789 start.go:364] duration metric: took 7.614¬µs to acquireMachinesLock for "minikube"
I0216 19:38:34.721265  246789 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:7700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0216 19:38:34.721303  246789 start.go:125] createHost starting for "" (driver="docker")
I0216 19:38:34.733770  246789 out.go:252] üî•  Creating docker container (CPUs=2, Memory=7700MB) ...
I0216 19:38:34.733853  246789 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0216 19:38:34.733860  246789 client.go:168] LocalClient.Create starting
I0216 19:38:34.733901  246789 main.go:141] libmachine: Creating CA: /home/lemonu/.minikube/certs/ca.pem
I0216 19:38:34.921380  246789 main.go:141] libmachine: Creating client certificate: /home/lemonu/.minikube/certs/cert.pem
I0216 19:38:35.094068  246789 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0216 19:38:35.098665  246789 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0216 19:38:35.098695  246789 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0216 19:38:35.098700  246789 cli_runner.go:164] Run: docker network inspect minikube
W0216 19:38:35.102725  246789 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0216 19:38:35.102732  246789 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0216 19:38:35.102736  246789 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0216 19:38:35.102781  246789 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0216 19:38:35.107172  246789 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001d773b0}
I0216 19:38:35.107185  246789 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0216 19:38:35.107205  246789 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0216 19:38:35.141450  246789 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0216 19:38:35.141462  246789 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0216 19:38:35.141506  246789 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0216 19:38:35.146335  246789 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0216 19:38:35.156274  246789 oci.go:103] Successfully created a docker volume minikube
I0216 19:38:35.156306  246789 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib
I0216 19:38:35.594437  246789 oci.go:107] Successfully prepared a docker volume minikube
I0216 19:38:35.594455  246789 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0216 19:38:35.594463  246789 kic.go:194] Starting extracting preloaded images to volume ...
I0216 19:38:35.594503  246789 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/lemonu/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir
I0216 19:38:37.533001  246789 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/lemonu/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir: (1.938481858s)
I0216 19:38:37.533012  246789 kic.go:203] duration metric: took 1.938547181s to extract preloaded images to volume ...
W0216 19:38:37.533052  246789 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0216 19:38:37.533066  246789 oci.go:252] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0216 19:38:37.533091  246789 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0216 19:38:37.555456  246789 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=7700mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1
I0216 19:38:37.751717  246789 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0216 19:38:37.756449  246789 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0216 19:38:37.761007  246789 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0216 19:38:37.774985  246789 oci.go:144] the created container "minikube" has a running status.
I0216 19:38:37.774995  246789 kic.go:225] Creating ssh key for kic: /home/lemonu/.minikube/machines/minikube/id_rsa...
I0216 19:38:38.036182  246789 kic_runner.go:191] docker (temp): /home/lemonu/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0216 19:38:38.043325  246789 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0216 19:38:38.048287  246789 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0216 19:38:38.048292  246789 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0216 19:38:38.065381  246789 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0216 19:38:38.070670  246789 machine.go:93] provisionDockerMachine start ...
I0216 19:38:38.070720  246789 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0216 19:38:38.075892  246789 main.go:141] libmachine: Using SSH client type: native
I0216 19:38:38.076006  246789 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x55a3d8e9dae0] 0x55a3d8ea07e0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0216 19:38:38.076008  246789 main.go:141] libmachine: About to run SSH command:
hostname
I0216 19:38:38.188005  246789 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0216 19:38:38.188013  246789 ubuntu.go:182] provisioning hostname "minikube"
I0216 19:38:38.188055  246789 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0216 19:38:38.193136  246789 main.go:141] libmachine: Using SSH client type: native
I0216 19:38:38.193228  246789 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x55a3d8e9dae0] 0x55a3d8ea07e0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0216 19:38:38.193231  246789 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0216 19:38:38.311107  246789 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0216 19:38:38.311150  246789 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0216 19:38:38.316468  246789 main.go:141] libmachine: Using SSH client type: native
I0216 19:38:38.316560  246789 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x55a3d8e9dae0] 0x55a3d8ea07e0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0216 19:38:38.316565  246789 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0216 19:38:38.428093  246789 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0216 19:38:38.428101  246789 ubuntu.go:188] set auth options {CertDir:/home/lemonu/.minikube CaCertPath:/home/lemonu/.minikube/certs/ca.pem CaPrivateKeyPath:/home/lemonu/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/lemonu/.minikube/machines/server.pem ServerKeyPath:/home/lemonu/.minikube/machines/server-key.pem ClientKeyPath:/home/lemonu/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/lemonu/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/lemonu/.minikube}
I0216 19:38:38.428110  246789 ubuntu.go:190] setting up certificates
I0216 19:38:38.428115  246789 provision.go:84] configureAuth start
I0216 19:38:38.428143  246789 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0216 19:38:38.433142  246789 provision.go:143] copyHostCerts
I0216 19:38:38.433171  246789 exec_runner.go:151] cp: /home/lemonu/.minikube/certs/ca.pem --> /home/lemonu/.minikube/ca.pem (1078 bytes)
I0216 19:38:38.433314  246789 exec_runner.go:151] cp: /home/lemonu/.minikube/certs/cert.pem --> /home/lemonu/.minikube/cert.pem (1119 bytes)
I0216 19:38:38.433349  246789 exec_runner.go:151] cp: /home/lemonu/.minikube/certs/key.pem --> /home/lemonu/.minikube/key.pem (1675 bytes)
I0216 19:38:38.433377  246789 provision.go:117] generating server cert: /home/lemonu/.minikube/machines/server.pem ca-key=/home/lemonu/.minikube/certs/ca.pem private-key=/home/lemonu/.minikube/certs/ca-key.pem org=lemonu.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0216 19:38:38.465358  246789 provision.go:177] copyRemoteCerts
I0216 19:38:38.465386  246789 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0216 19:38:38.465406  246789 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0216 19:38:38.470320  246789 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/lemonu/.minikube/machines/minikube/id_rsa Username:docker}
I0216 19:38:38.551056  246789 ssh_runner.go:362] scp /home/lemonu/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0216 19:38:38.563521  246789 ssh_runner.go:362] scp /home/lemonu/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0216 19:38:38.575971  246789 ssh_runner.go:362] scp /home/lemonu/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I0216 19:38:38.588425  246789 provision.go:87] duration metric: took 160.30715ms to configureAuth
I0216 19:38:38.588431  246789 ubuntu.go:206] setting minikube options for container-runtime
I0216 19:38:38.588498  246789 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0216 19:38:38.588527  246789 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0216 19:38:38.593781  246789 main.go:141] libmachine: Using SSH client type: native
I0216 19:38:38.593889  246789 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x55a3d8e9dae0] 0x55a3d8ea07e0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0216 19:38:38.593892  246789 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0216 19:38:38.705217  246789 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0216 19:38:38.705223  246789 ubuntu.go:71] root file system type: overlay
I0216 19:38:38.705266  246789 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0216 19:38:38.705306  246789 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0216 19:38:38.710400  246789 main.go:141] libmachine: Using SSH client type: native
I0216 19:38:38.710491  246789 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x55a3d8e9dae0] 0x55a3d8ea07e0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0216 19:38:38.710515  246789 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0216 19:38:38.827417  246789 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0216 19:38:38.827451  246789 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0216 19:38:38.832568  246789 main.go:141] libmachine: Using SSH client type: native
I0216 19:38:38.832660  246789 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x55a3d8e9dae0] 0x55a3d8ea07e0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0216 19:38:38.832665  246789 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0216 19:38:39.307821  246789 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-09-03 20:55:49.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2026-02-17 00:38:38.826748615 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0216 19:38:39.307830  246789 machine.go:96] duration metric: took 1.237151569s to provisionDockerMachine
I0216 19:38:39.307836  246789 client.go:171] duration metric: took 4.57397338s to LocalClient.Create
I0216 19:38:39.307842  246789 start.go:167] duration metric: took 4.57398891s to libmachine.API.Create "minikube"
I0216 19:38:39.307845  246789 start.go:293] postStartSetup for "minikube" (driver="docker")
I0216 19:38:39.307849  246789 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0216 19:38:39.307884  246789 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0216 19:38:39.307910  246789 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0216 19:38:39.313924  246789 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/lemonu/.minikube/machines/minikube/id_rsa Username:docker}
I0216 19:38:39.394118  246789 ssh_runner.go:195] Run: cat /etc/os-release
I0216 19:38:39.395791  246789 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0216 19:38:39.395802  246789 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0216 19:38:39.395807  246789 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0216 19:38:39.395810  246789 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0216 19:38:39.395814  246789 filesync.go:126] Scanning /home/lemonu/.minikube/addons for local assets ...
I0216 19:38:39.395844  246789 filesync.go:126] Scanning /home/lemonu/.minikube/files for local assets ...
I0216 19:38:39.395859  246789 start.go:296] duration metric: took 88.011515ms for postStartSetup
I0216 19:38:39.396090  246789 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0216 19:38:39.401221  246789 profile.go:143] Saving config to /home/lemonu/.minikube/profiles/minikube/config.json ...
I0216 19:38:39.401339  246789 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0216 19:38:39.401360  246789 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0216 19:38:39.405846  246789 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/lemonu/.minikube/machines/minikube/id_rsa Username:docker}
I0216 19:38:39.484846  246789 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0216 19:38:39.486726  246789 start.go:128] duration metric: took 4.765418494s to createHost
I0216 19:38:39.486730  246789 start.go:83] releasing machines lock for "minikube", held for 4.765474198s
I0216 19:38:39.486758  246789 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0216 19:38:39.491690  246789 ssh_runner.go:195] Run: cat /version.json
I0216 19:38:39.491715  246789 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0216 19:38:39.491729  246789 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0216 19:38:39.491762  246789 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0216 19:38:39.496126  246789 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/lemonu/.minikube/machines/minikube/id_rsa Username:docker}
I0216 19:38:39.496498  246789 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/lemonu/.minikube/machines/minikube/id_rsa Username:docker}
I0216 19:38:39.766778  246789 ssh_runner.go:195] Run: systemctl --version
I0216 19:38:39.769122  246789 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0216 19:38:39.771193  246789 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0216 19:38:39.784223  246789 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0216 19:38:39.784252  246789 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0216 19:38:39.796074  246789 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0216 19:38:39.796078  246789 start.go:495] detecting cgroup driver to use...
I0216 19:38:39.796089  246789 detect.go:190] detected "systemd" cgroup driver on host os
I0216 19:38:39.796128  246789 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0216 19:38:39.804266  246789 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I0216 19:38:39.809102  246789 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0216 19:38:39.813858  246789 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0216 19:38:39.813891  246789 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0216 19:38:39.818605  246789 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0216 19:38:39.823292  246789 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0216 19:38:39.828337  246789 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0216 19:38:39.833016  246789 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0216 19:38:39.837582  246789 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0216 19:38:39.842594  246789 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0216 19:38:39.847292  246789 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0216 19:38:39.852387  246789 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0216 19:38:39.857020  246789 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0216 19:38:39.861210  246789 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0216 19:38:39.896843  246789 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0216 19:38:39.943160  246789 start.go:495] detecting cgroup driver to use...
I0216 19:38:39.943180  246789 detect.go:190] detected "systemd" cgroup driver on host os
I0216 19:38:39.943222  246789 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0216 19:38:39.949358  246789 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0216 19:38:39.955224  246789 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0216 19:38:39.962745  246789 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0216 19:38:39.968193  246789 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0216 19:38:39.973825  246789 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0216 19:38:39.982495  246789 ssh_runner.go:195] Run: which cri-dockerd
I0216 19:38:39.984300  246789 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0216 19:38:39.988629  246789 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I0216 19:38:39.997839  246789 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0216 19:38:40.038142  246789 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0216 19:38:40.073714  246789 docker.go:575] configuring docker to use "systemd" as cgroup driver...
I0216 19:38:40.073754  246789 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0216 19:38:40.083450  246789 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0216 19:38:40.088786  246789 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0216 19:38:40.127345  246789 ssh_runner.go:195] Run: sudo systemctl restart docker
I0216 19:38:40.359894  246789 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I0216 19:38:40.365719  246789 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0216 19:38:40.371956  246789 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0216 19:38:40.377554  246789 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0216 19:38:40.417744  246789 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0216 19:38:40.455478  246789 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0216 19:38:40.494476  246789 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0216 19:38:40.516439  246789 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0216 19:38:40.521800  246789 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0216 19:38:40.562156  246789 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0216 19:38:40.597453  246789 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0216 19:38:40.603902  246789 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0216 19:38:40.603931  246789 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0216 19:38:40.605740  246789 start.go:563] Will wait 60s for crictl version
I0216 19:38:40.605760  246789 ssh_runner.go:195] Run: which crictl
I0216 19:38:40.607430  246789 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0216 19:38:40.619976  246789 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I0216 19:38:40.620005  246789 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0216 19:38:40.629056  246789 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0216 19:38:40.643284  246789 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I0216 19:38:40.643337  246789 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0216 19:38:40.648274  246789 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0216 19:38:40.650204  246789 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0216 19:38:40.656232  246789 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:7700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0216 19:38:40.656278  246789 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0216 19:38:40.656306  246789 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0216 19:38:40.664748  246789 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0216 19:38:40.664753  246789 docker.go:621] Images already preloaded, skipping extraction
I0216 19:38:40.664782  246789 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0216 19:38:40.671878  246789 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0216 19:38:40.671883  246789 cache_images.go:85] Images are preloaded, skipping loading
I0216 19:38:40.671887  246789 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I0216 19:38:40.671926  246789 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0216 19:38:40.671959  246789 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0216 19:38:40.689822  246789 cni.go:84] Creating CNI manager for ""
I0216 19:38:40.689831  246789 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0216 19:38:40.689837  246789 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0216 19:38:40.689844  246789 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0216 19:38:40.689893  246789 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0216 19:38:40.689928  246789 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I0216 19:38:40.694544  246789 binaries.go:44] Found k8s binaries, skipping transfer
I0216 19:38:40.694571  246789 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0216 19:38:40.698937  246789 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0216 19:38:40.708290  246789 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0216 19:38:40.717717  246789 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I0216 19:38:40.726684  246789 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0216 19:38:40.728329  246789 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0216 19:38:40.733557  246789 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0216 19:38:40.773680  246789 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0216 19:38:40.790211  246789 certs.go:68] Setting up /home/lemonu/.minikube/profiles/minikube for IP: 192.168.49.2
I0216 19:38:40.790215  246789 certs.go:194] generating shared ca certs ...
I0216 19:38:40.790222  246789 certs.go:226] acquiring lock for ca certs: {Name:mkce7e088f810c4ecb4b2b6b0cd9dadf7f6305a4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0216 19:38:40.790270  246789 certs.go:240] generating "minikubeCA" ca cert: /home/lemonu/.minikube/ca.key
I0216 19:38:40.846596  246789 crypto.go:156] Writing cert to /home/lemonu/.minikube/ca.crt ...
I0216 19:38:40.846602  246789 lock.go:35] WriteFile acquiring /home/lemonu/.minikube/ca.crt: {Name:mk2348c9ee090e27d346916a4a8a21eb48875950 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0216 19:38:40.846666  246789 crypto.go:164] Writing key to /home/lemonu/.minikube/ca.key ...
I0216 19:38:40.846668  246789 lock.go:35] WriteFile acquiring /home/lemonu/.minikube/ca.key: {Name:mk56d7606a9da1dee704bd35f7b570d115ebc533 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0216 19:38:40.846699  246789 certs.go:240] generating "proxyClientCA" ca cert: /home/lemonu/.minikube/proxy-client-ca.key
I0216 19:38:40.950127  246789 crypto.go:156] Writing cert to /home/lemonu/.minikube/proxy-client-ca.crt ...
I0216 19:38:40.950130  246789 lock.go:35] WriteFile acquiring /home/lemonu/.minikube/proxy-client-ca.crt: {Name:mk6c66f5c8754e7f39ad050c55a0eeae28827d74 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0216 19:38:40.950176  246789 crypto.go:164] Writing key to /home/lemonu/.minikube/proxy-client-ca.key ...
I0216 19:38:40.950178  246789 lock.go:35] WriteFile acquiring /home/lemonu/.minikube/proxy-client-ca.key: {Name:mke21447b43debd68943a7b9ae63286e6596e4ff Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0216 19:38:40.950204  246789 certs.go:256] generating profile certs ...
I0216 19:38:40.950229  246789 certs.go:363] generating signed profile cert for "minikube-user": /home/lemonu/.minikube/profiles/minikube/client.key
I0216 19:38:40.950236  246789 crypto.go:68] Generating cert /home/lemonu/.minikube/profiles/minikube/client.crt with IP's: []
I0216 19:38:41.144788  246789 crypto.go:156] Writing cert to /home/lemonu/.minikube/profiles/minikube/client.crt ...
I0216 19:38:41.144791  246789 lock.go:35] WriteFile acquiring /home/lemonu/.minikube/profiles/minikube/client.crt: {Name:mkfe31a4dd5a608d9fb9968125366a8ebd4ef9c8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0216 19:38:41.144832  246789 crypto.go:164] Writing key to /home/lemonu/.minikube/profiles/minikube/client.key ...
I0216 19:38:41.144834  246789 lock.go:35] WriteFile acquiring /home/lemonu/.minikube/profiles/minikube/client.key: {Name:mk7ca2930950dcfe79a58378fec4055e558c2742 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0216 19:38:41.144864  246789 certs.go:363] generating signed profile cert for "minikube": /home/lemonu/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0216 19:38:41.144872  246789 crypto.go:68] Generating cert /home/lemonu/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0216 19:38:41.258715  246789 crypto.go:156] Writing cert to /home/lemonu/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0216 19:38:41.258718  246789 lock.go:35] WriteFile acquiring /home/lemonu/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mkb9210ab123668e0328c5d3f7d8e20bdaf6045a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0216 19:38:41.258758  246789 crypto.go:164] Writing key to /home/lemonu/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0216 19:38:41.258760  246789 lock.go:35] WriteFile acquiring /home/lemonu/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mkacc32928123275fbefccd7c25ef9e617fd2a3c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0216 19:38:41.258786  246789 certs.go:381] copying /home/lemonu/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/lemonu/.minikube/profiles/minikube/apiserver.crt
I0216 19:38:41.258876  246789 certs.go:385] copying /home/lemonu/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/lemonu/.minikube/profiles/minikube/apiserver.key
I0216 19:38:41.258966  246789 certs.go:363] generating signed profile cert for "aggregator": /home/lemonu/.minikube/profiles/minikube/proxy-client.key
I0216 19:38:41.258975  246789 crypto.go:68] Generating cert /home/lemonu/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0216 19:38:41.282057  246789 crypto.go:156] Writing cert to /home/lemonu/.minikube/profiles/minikube/proxy-client.crt ...
I0216 19:38:41.282060  246789 lock.go:35] WriteFile acquiring /home/lemonu/.minikube/profiles/minikube/proxy-client.crt: {Name:mk68e37b75a7cd37e9e86e7b218719b60c169654 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0216 19:38:41.282098  246789 crypto.go:164] Writing key to /home/lemonu/.minikube/profiles/minikube/proxy-client.key ...
I0216 19:38:41.282100  246789 lock.go:35] WriteFile acquiring /home/lemonu/.minikube/profiles/minikube/proxy-client.key: {Name:mk4ed7dc1fbc52fa0bc848afed4db5f6b06ff981 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0216 19:38:41.282182  246789 certs.go:484] found cert: /home/lemonu/.minikube/certs/ca-key.pem (1675 bytes)
I0216 19:38:41.282195  246789 certs.go:484] found cert: /home/lemonu/.minikube/certs/ca.pem (1078 bytes)
I0216 19:38:41.282206  246789 certs.go:484] found cert: /home/lemonu/.minikube/certs/cert.pem (1119 bytes)
I0216 19:38:41.282216  246789 certs.go:484] found cert: /home/lemonu/.minikube/certs/key.pem (1675 bytes)
I0216 19:38:41.282419  246789 ssh_runner.go:362] scp /home/lemonu/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0216 19:38:41.295832  246789 ssh_runner.go:362] scp /home/lemonu/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0216 19:38:41.307992  246789 ssh_runner.go:362] scp /home/lemonu/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0216 19:38:41.321312  246789 ssh_runner.go:362] scp /home/lemonu/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0216 19:38:41.333892  246789 ssh_runner.go:362] scp /home/lemonu/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0216 19:38:41.346189  246789 ssh_runner.go:362] scp /home/lemonu/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0216 19:38:41.358369  246789 ssh_runner.go:362] scp /home/lemonu/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0216 19:38:41.370716  246789 ssh_runner.go:362] scp /home/lemonu/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0216 19:38:41.383139  246789 ssh_runner.go:362] scp /home/lemonu/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0216 19:38:41.395870  246789 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0216 19:38:41.404864  246789 ssh_runner.go:195] Run: openssl version
I0216 19:38:41.407366  246789 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0216 19:38:41.412338  246789 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0216 19:38:41.414043  246789 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Feb 17 00:38 /usr/share/ca-certificates/minikubeCA.pem
I0216 19:38:41.414079  246789 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0216 19:38:41.417272  246789 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0216 19:38:41.421974  246789 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0216 19:38:41.423600  246789 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0216 19:38:41.423617  246789 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:7700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0216 19:38:41.423673  246789 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0216 19:38:41.430705  246789 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0216 19:38:41.435255  246789 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0216 19:38:41.439673  246789 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0216 19:38:41.439701  246789 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0216 19:38:41.444141  246789 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0216 19:38:41.444143  246789 kubeadm.go:157] found existing configuration files:

I0216 19:38:41.444162  246789 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0216 19:38:41.448496  246789 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0216 19:38:41.448520  246789 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0216 19:38:41.452940  246789 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0216 19:38:41.457416  246789 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0216 19:38:41.457440  246789 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0216 19:38:41.461820  246789 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0216 19:38:41.466350  246789 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0216 19:38:41.466372  246789 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0216 19:38:41.470398  246789 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0216 19:38:41.474450  246789 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0216 19:38:41.474472  246789 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0216 19:38:41.478566  246789 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0216 19:38:41.492505  246789 kubeadm.go:310] [init] Using Kubernetes version: v1.34.0
I0216 19:38:41.492523  246789 kubeadm.go:310] [preflight] Running pre-flight checks
I0216 19:38:41.536760  246789 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0216 19:38:41.536815  246789 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0216 19:38:41.536867  246789 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0216 19:38:41.540732  246789 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0216 19:38:41.546210  246789 out.go:252]     ‚ñ™ Generating certificates and keys ...
I0216 19:38:41.546280  246789 kubeadm.go:310] [certs] Using existing ca certificate authority
I0216 19:38:41.546306  246789 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0216 19:38:41.580056  246789 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0216 19:38:41.700597  246789 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0216 19:38:41.820523  246789 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0216 19:38:41.873511  246789 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0216 19:38:42.099209  246789 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0216 19:38:42.099275  246789 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0216 19:38:42.226456  246789 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0216 19:38:42.226510  246789 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0216 19:38:42.261951  246789 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0216 19:38:42.362537  246789 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0216 19:38:42.413016  246789 kubeadm.go:310] [certs] Generating "sa" key and public key
I0216 19:38:42.413041  246789 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0216 19:38:42.757291  246789 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0216 19:38:42.866172  246789 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0216 19:38:43.054904  246789 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0216 19:38:43.130071  246789 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0216 19:38:43.162820  246789 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0216 19:38:43.162892  246789 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0216 19:38:43.163735  246789 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0216 19:38:43.169432  246789 out.go:252]     ‚ñ™ Booting up control plane ...
I0216 19:38:43.169470  246789 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0216 19:38:43.169494  246789 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0216 19:38:43.169519  246789 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0216 19:38:43.169553  246789 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0216 19:38:43.169600  246789 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
I0216 19:38:43.172131  246789 kubeadm.go:310] [patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
I0216 19:38:43.172279  246789 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0216 19:38:43.172293  246789 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0216 19:38:43.223786  246789 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0216 19:38:43.223827  246789 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0216 19:38:43.724877  246789 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.134551ms
I0216 19:38:43.726190  246789 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0216 19:38:43.726228  246789 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I0216 19:38:43.726270  246789 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0216 19:38:43.726296  246789 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0216 19:38:44.728475  246789 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 1.002155309s
I0216 19:38:44.978554  246789 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 1.252306839s
I0216 19:38:46.727744  246789 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 3.001403043s
I0216 19:38:46.734296  246789 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0216 19:38:46.744662  246789 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0216 19:38:46.754973  246789 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0216 19:38:46.755034  246789 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0216 19:38:46.759828  246789 kubeadm.go:310] [bootstrap-token] Using token: bzw8vn.gysmhty00jzqitzv
I0216 19:38:46.762269  246789 out.go:252]     ‚ñ™ Configuring RBAC rules ...
I0216 19:38:46.762325  246789 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0216 19:38:46.765210  246789 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0216 19:38:46.773711  246789 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0216 19:38:46.777045  246789 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0216 19:38:46.780472  246789 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0216 19:38:46.783898  246789 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0216 19:38:47.132342  246789 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0216 19:38:47.553017  246789 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0216 19:38:48.132711  246789 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0216 19:38:48.133127  246789 kubeadm.go:310] 
I0216 19:38:48.133152  246789 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0216 19:38:48.133154  246789 kubeadm.go:310] 
I0216 19:38:48.133179  246789 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0216 19:38:48.133181  246789 kubeadm.go:310] 
I0216 19:38:48.133192  246789 kubeadm.go:310]   mkdir -p $HOME/.kube
I0216 19:38:48.133211  246789 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0216 19:38:48.133227  246789 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0216 19:38:48.133228  246789 kubeadm.go:310] 
I0216 19:38:48.133251  246789 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0216 19:38:48.133253  246789 kubeadm.go:310] 
I0216 19:38:48.133278  246789 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0216 19:38:48.133280  246789 kubeadm.go:310] 
I0216 19:38:48.133298  246789 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0216 19:38:48.133322  246789 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0216 19:38:48.133343  246789 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0216 19:38:48.133344  246789 kubeadm.go:310] 
I0216 19:38:48.133371  246789 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0216 19:38:48.133395  246789 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0216 19:38:48.133397  246789 kubeadm.go:310] 
I0216 19:38:48.133423  246789 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token bzw8vn.gysmhty00jzqitzv \
I0216 19:38:48.133462  246789 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:8921371b13d7e3cd33d7cb09faa2d759484abe06186490e42db3ff22c70cff1f \
I0216 19:38:48.133469  246789 kubeadm.go:310] 	--control-plane 
I0216 19:38:48.133470  246789 kubeadm.go:310] 
I0216 19:38:48.133498  246789 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0216 19:38:48.133504  246789 kubeadm.go:310] 
I0216 19:38:48.133540  246789 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token bzw8vn.gysmhty00jzqitzv \
I0216 19:38:48.133581  246789 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:8921371b13d7e3cd33d7cb09faa2d759484abe06186490e42db3ff22c70cff1f 
I0216 19:38:48.134972  246789 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0216 19:38:48.135013  246789 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0216 19:38:48.135019  246789 cni.go:84] Creating CNI manager for ""
I0216 19:38:48.135024  246789 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0216 19:38:48.137807  246789 out.go:179] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0216 19:38:48.140571  246789 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0216 19:38:48.145278  246789 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0216 19:38:48.154584  246789 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0216 19:38:48.154617  246789 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0216 19:38:48.154618  246789 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2026_02_16T19_38_48_0700 minikube.k8s.io/version=v1.37.0 minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0216 19:38:48.158714  246789 ops.go:34] apiserver oom_adj: -16
I0216 19:38:48.194915  246789 kubeadm.go:1105] duration metric: took 40.327688ms to wait for elevateKubeSystemPrivileges
I0216 19:38:48.194922  246789 kubeadm.go:394] duration metric: took 6.771307434s to StartCluster
I0216 19:38:48.194930  246789 settings.go:142] acquiring lock: {Name:mk3281f3f813fc4c2df9cc1b725db475f867564b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0216 19:38:48.194956  246789 settings.go:150] Updating kubeconfig:  /home/lemonu/.kube/config
I0216 19:38:48.195196  246789 lock.go:35] WriteFile acquiring /home/lemonu/.kube/config: {Name:mk9f58b6ed5a90543f129b01483a5772841cf661 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0216 19:38:48.195283  246789 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0216 19:38:48.195288  246789 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0216 19:38:48.195305  246789 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0216 19:38:48.195332  246789 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0216 19:38:48.195340  246789 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0216 19:38:48.195349  246789 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0216 19:38:48.195352  246789 host.go:66] Checking if "minikube" exists ...
I0216 19:38:48.195355  246789 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0216 19:38:48.195373  246789 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0216 19:38:48.195541  246789 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0216 19:38:48.195591  246789 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0216 19:38:48.198242  246789 out.go:179] üîé  Verifying Kubernetes components...
I0216 19:38:48.201070  246789 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0216 19:38:48.201401  246789 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0216 19:38:48.201412  246789 host.go:66] Checking if "minikube" exists ...
I0216 19:38:48.201569  246789 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0216 19:38:48.203629  246789 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0216 19:38:48.206552  246789 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0216 19:38:48.206557  246789 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0216 19:38:48.206601  246789 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0216 19:38:48.207020  246789 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0216 19:38:48.207025  246789 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0216 19:38:48.207047  246789 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0216 19:38:48.212574  246789 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/lemonu/.minikube/machines/minikube/id_rsa Username:docker}
I0216 19:38:48.212999  246789 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/lemonu/.minikube/machines/minikube/id_rsa Username:docker}
I0216 19:38:48.226877  246789 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0216 19:38:48.258017  246789 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0216 19:38:48.285865  246789 start.go:976] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I0216 19:38:48.286149  246789 api_server.go:52] waiting for apiserver process to appear ...
I0216 19:38:48.286181  246789 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0216 19:38:48.292631  246789 api_server.go:72] duration metric: took 97.33255ms to wait for apiserver process to appear ...
I0216 19:38:48.292637  246789 api_server.go:88] waiting for apiserver healthz status ...
I0216 19:38:48.292643  246789 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0216 19:38:48.294322  246789 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0216 19:38:48.294599  246789 api_server.go:141] control plane version: v1.34.0
I0216 19:38:48.294603  246789 api_server.go:131] duration metric: took 1.964079ms to wait for apiserver health ...
I0216 19:38:48.294606  246789 system_pods.go:43] waiting for kube-system pods to appear ...
I0216 19:38:48.298607  246789 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0216 19:38:48.299122  246789 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0216 19:38:48.304915  246789 system_pods.go:59] 4 kube-system pods found
I0216 19:38:48.304927  246789 system_pods.go:61] "etcd-minikube" [785dd253-6ae9-4ab4-82ec-6c5a4bb98152] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0216 19:38:48.304930  246789 system_pods.go:61] "kube-apiserver-minikube" [0b6291bb-18f6-4b90-90c2-5f640de14355] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0216 19:38:48.304933  246789 system_pods.go:61] "kube-controller-manager-minikube" [705ba675-6df8-4a22-8d45-95ae560f0824] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0216 19:38:48.304935  246789 system_pods.go:61] "kube-scheduler-minikube" [55e09789-78bf-4b58-9bca-65b8a6b997db] Running
I0216 19:38:48.304938  246789 system_pods.go:74] duration metric: took 10.328976ms to wait for pod list to return data ...
I0216 19:38:48.304943  246789 kubeadm.go:578] duration metric: took 109.646925ms to wait for: map[apiserver:true system_pods:true]
I0216 19:38:48.304949  246789 node_conditions.go:102] verifying NodePressure condition ...
I0216 19:38:48.305925  246789 node_conditions.go:122] node storage ephemeral capacity is 974696Mi
I0216 19:38:48.305934  246789 node_conditions.go:123] node cpu capacity is 12
I0216 19:38:48.305942  246789 node_conditions.go:105] duration metric: took 990.451¬µs to run NodePressure ...
I0216 19:38:48.305949  246789 start.go:241] waiting for startup goroutines ...
I0216 19:38:48.425511  246789 out.go:179] üåü  Enabled addons: default-storageclass, storage-provisioner
I0216 19:38:48.428209  246789 addons.go:514] duration metric: took 232.904093ms for enable addons: enabled=[default-storageclass storage-provisioner]
I0216 19:38:48.787637  246789 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0216 19:38:48.787647  246789 start.go:246] waiting for cluster config update ...
I0216 19:38:48.787652  246789 start.go:255] writing updated cluster config ...
I0216 19:38:48.787784  246789 ssh_runner.go:195] Run: rm -f paused
I0216 19:38:48.821491  246789 start.go:617] kubectl: 1.35.0, cluster: 1.34.0 (minor skew: 1)
I0216 19:38:48.824328  246789 out.go:179] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Feb 17 00:38:39 minikube systemd[1]: Started Docker Application Container Engine.
Feb 17 00:38:39 minikube dockerd[691]: time="2026-02-17T00:38:39.902531120Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=moby
Feb 17 00:38:39 minikube dockerd[691]: time="2026-02-17T00:38:39.902550707Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=moby
Feb 17 00:38:39 minikube dockerd[691]: time="2026-02-17T00:38:39.902531060Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=plugins.moby
Feb 17 00:38:39 minikube dockerd[691]: time="2026-02-17T00:38:39.902585322Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=plugins.moby
Feb 17 00:38:39 minikube dockerd[691]: time="2026-02-17T00:38:39.960457361Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=plugins.moby
Feb 17 00:38:39 minikube dockerd[691]: time="2026-02-17T00:38:39.960469674Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=plugins.moby
Feb 17 00:38:39 minikube dockerd[691]: time="2026-02-17T00:38:39.960466799Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=moby
Feb 17 00:38:39 minikube dockerd[691]: time="2026-02-17T00:38:39.960505241Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=moby
Feb 17 00:38:40 minikube systemd[1]: Stopping Docker Application Container Engine...
Feb 17 00:38:40 minikube dockerd[691]: time="2026-02-17T00:38:40.132833844Z" level=info msg="Processing signal 'terminated'"
Feb 17 00:38:40 minikube dockerd[691]: time="2026-02-17T00:38:40.133335321Z" level=warning msg="Error while testing if containerd API is ready" error="Canceled: grpc: the client connection is closing"
Feb 17 00:38:40 minikube dockerd[691]: time="2026-02-17T00:38:40.133474143Z" level=info msg="Daemon shutdown complete"
Feb 17 00:38:40 minikube dockerd[691]: time="2026-02-17T00:38:40.133477981Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=moby
Feb 17 00:38:40 minikube dockerd[691]: time="2026-02-17T00:38:40.133492338Z" level=warning msg="Error while testing if containerd API is ready" error="Canceled: context canceled while waiting for connections to become ready"
Feb 17 00:38:40 minikube systemd[1]: docker.service: Deactivated successfully.
Feb 17 00:38:40 minikube systemd[1]: Stopped Docker Application Container Engine.
Feb 17 00:38:40 minikube systemd[1]: Starting Docker Application Container Engine...
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.183327008Z" level=info msg="Starting up"
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.183709450Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.183749195Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/etc/cdi
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.183754545Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/var/run/cdi
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.187820093Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.205411043Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.217614067Z" level=info msg="Loading containers: start."
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.304489738Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 29755d8fff67e75f31441f1fde481cde55049d566daf0e79c3a8072c14762b79], retrying...."
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.345995379Z" level=info msg="Loading containers: done."
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.354627991Z" level=warning msg="Not using native diff for overlay2, this may cause degraded performance for building images: kernel has CONFIG_OVERLAY_FS_REDIRECT_DIR enabled" storage-driver=overlay2
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.354664510Z" level=info msg="Docker daemon" commit=249d679 containerd-snapshotter=false storage-driver=overlay2 version=28.4.0
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.354681462Z" level=info msg="Initializing buildkit"
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.356902837Z" level=info msg="Completed buildkit initialization"
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.358879820Z" level=info msg="Daemon has completed initialization"
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.358901642Z" level=info msg="API listen on /var/run/docker.sock"
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.358943982Z" level=info msg="API listen on /run/docker.sock"
Feb 17 00:38:40 minikube dockerd[1138]: time="2026-02-17T00:38:40.358955754Z" level=info msg="API listen on [::]:2376"
Feb 17 00:38:40 minikube systemd[1]: Started Docker Application Container Engine.
Feb 17 00:38:40 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Feb 17 00:38:40 minikube cri-dockerd[1449]: time="2026-02-17T00:38:40Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Feb 17 00:38:40 minikube cri-dockerd[1449]: time="2026-02-17T00:38:40Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Feb 17 00:38:40 minikube cri-dockerd[1449]: time="2026-02-17T00:38:40Z" level=info msg="Start docker client with request timeout 0s"
Feb 17 00:38:40 minikube cri-dockerd[1449]: time="2026-02-17T00:38:40Z" level=info msg="Hairpin mode is set to hairpin-veth"
Feb 17 00:38:40 minikube cri-dockerd[1449]: time="2026-02-17T00:38:40Z" level=info msg="Loaded network plugin cni"
Feb 17 00:38:40 minikube cri-dockerd[1449]: time="2026-02-17T00:38:40Z" level=info msg="Docker cri networking managed by network plugin cni"
Feb 17 00:38:40 minikube cri-dockerd[1449]: time="2026-02-17T00:38:40Z" level=info msg="Setting cgroupDriver systemd"
Feb 17 00:38:40 minikube cri-dockerd[1449]: time="2026-02-17T00:38:40Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Feb 17 00:38:40 minikube cri-dockerd[1449]: time="2026-02-17T00:38:40Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Feb 17 00:38:40 minikube cri-dockerd[1449]: time="2026-02-17T00:38:40Z" level=info msg="Start cri-dockerd grpc backend"
Feb 17 00:38:40 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Feb 17 00:38:44 minikube cri-dockerd[1449]: time="2026-02-17T00:38:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/65c02221e5956be47cae680516ab9010990fc3882d297357cc32fb8b965caa41/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 17 00:38:44 minikube cri-dockerd[1449]: time="2026-02-17T00:38:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0b5bc0a0ae0d9a5e63473cc8ecedfee7da1adc4cc0f12ca6ac5601d2e416c9ce/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 17 00:38:44 minikube cri-dockerd[1449]: time="2026-02-17T00:38:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/52abf1441460f13dd69c1ce5a3bef8406c5917a188a466ba11019fa119392d9d/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 17 00:38:44 minikube cri-dockerd[1449]: time="2026-02-17T00:38:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cbc551bf0042ea2e666a806a92bad7e23232fc33c16c5b8beb2e29aaab58fa03/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 17 00:38:53 minikube cri-dockerd[1449]: time="2026-02-17T00:38:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0f636d8579da64d02bedbd9ae69adf5ce87b6bfc19db80d5f0397dc6d52d7824/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 17 00:38:53 minikube cri-dockerd[1449]: time="2026-02-17T00:38:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dfec4a87e41c6eda092b4bfb5911855daa25b03dce1d0798c833165b88fc21ea/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 17 00:38:53 minikube cri-dockerd[1449]: time="2026-02-17T00:38:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/76a79a82009dc2062f84c63d208d708a5159fa3bb404311915112f2d65233541/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 17 00:38:57 minikube cri-dockerd[1449]: time="2026-02-17T00:38:57Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 17 00:42:19 minikube cri-dockerd[1449]: time="2026-02-17T00:42:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d0d42075d87f0e29062f4e9c131547f592f736361804bdb31d0ada3e2f9a9edd/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 17 00:42:19 minikube cri-dockerd[1449]: time="2026-02-17T00:42:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f88b4ec2f4dd3ed22f9c3f1850329debfb61d2572494cd60eb8d291426c1ba4d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 17 00:42:26 minikube cri-dockerd[1449]: time="2026-02-17T00:42:26Z" level=info msg="Stop pulling image shreesu/kubernetes-demo-api:latest: Status: Downloaded newer image for shreesu/kubernetes-demo-api:latest"
Feb 17 00:42:27 minikube cri-dockerd[1449]: time="2026-02-17T00:42:27Z" level=info msg="Stop pulling image shreesu/kubernetes-demo-api:latest: Status: Image is up to date for shreesu/kubernetes-demo-api:latest"


==> container status <==
CONTAINER           IMAGE                                                                                                 CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
fc8ff2fed1e2a       shreesu/kubernetes-demo-api@sha256:0c656f66e93f302ee4c50484b2ba807fc0f35dc2432fb491865c39a3c010f4cc   2 minutes ago       Running             kubernetes-demo-api       0                   f88b4ec2f4dd3       kubernetes-demo-api-94ddfddc4-dq6wt
1ea66f4551df6       shreesu/kubernetes-demo-api@sha256:0c656f66e93f302ee4c50484b2ba807fc0f35dc2432fb491865c39a3c010f4cc   2 minutes ago       Running             kubernetes-demo-api       0                   d0d42075d87f0       kubernetes-demo-api-94ddfddc4-j9wkc
ff05d7a6cfebc       6e38f40d628db                                                                                         6 minutes ago       Running             storage-provisioner       0                   76a79a82009dc       storage-provisioner
94a93b94fe700       52546a367cc9e                                                                                         6 minutes ago       Running             coredns                   0                   dfec4a87e41c6       coredns-66bc5c9577-d6lcs
78d5126b03e99       df0860106674d                                                                                         6 minutes ago       Running             kube-proxy                0                   0f636d8579da6       kube-proxy-96xwg
8d8ef3bbbf951       90550c43ad2bc                                                                                         6 minutes ago       Running             kube-apiserver            0                   cbc551bf0042e       kube-apiserver-minikube
96a99086fa2fd       5f1f5298c888d                                                                                         6 minutes ago       Running             etcd                      0                   52abf1441460f       etcd-minikube
3ae466c294aa0       46169d968e920                                                                                         6 minutes ago       Running             kube-scheduler            0                   0b5bc0a0ae0d9       kube-scheduler-minikube
6001d55ccb262       a0af72f2ec6d6                                                                                         6 minutes ago       Running             kube-controller-manager   0                   65c02221e5956       kube-controller-manager-minikube


==> coredns [94a93b94fe70] <==
maxprocs: Leaving GOMAXPROCS=12: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:52653 - 63634 "HINFO IN 2638712084899903211.8425834471309130687. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.014607045s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] 10.244.0.3:41836 - 16730 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000124863s
[INFO] 10.244.0.3:41836 - 16550 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000154262s
[INFO] 10.244.0.3:56518 - 22269 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.00004623s
[INFO] 10.244.0.3:56518 - 21812 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000057683s
[INFO] 10.244.0.3:43180 - 411 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000051962s
[INFO] 10.244.0.3:43180 - 15 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.00005598s
[INFO] 10.244.0.3:56147 - 43434 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.010435563s
[INFO] 10.244.0.3:56147 - 43564 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 588 0.010829122s
[INFO] 10.244.0.4:49743 - 30316 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000126297s
[INFO] 10.244.0.4:49743 - 30105 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000160724s
[INFO] 10.244.0.4:60254 - 43479 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000042252s
[INFO] 10.244.0.4:60254 - 43103 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000056069s
[INFO] 10.244.0.4:52017 - 7508 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000030168s
[INFO] 10.244.0.4:52017 - 7408 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000046421s
[INFO] 10.244.0.4:56574 - 35116 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 588 0.000036271s
[INFO] 10.244.0.4:56574 - 35046 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 444 0.000048805s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2026_02_16T19_38_48_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 17 Feb 2026 00:38:45 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 17 Feb 2026 00:45:14 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 17 Feb 2026 00:42:52 +0000   Tue, 17 Feb 2026 00:38:45 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 17 Feb 2026 00:42:52 +0000   Tue, 17 Feb 2026 00:38:45 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 17 Feb 2026 00:42:52 +0000   Tue, 17 Feb 2026 00:38:45 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 17 Feb 2026 00:42:52 +0000   Tue, 17 Feb 2026 00:38:47 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  974696Mi
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             31941540Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  974696Mi
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             31941540Ki
  pods:               110
System Info:
  Machine ID:                 d46c6fef8af8453aa015500676939134
  System UUID:                ab27230b-fb93-4957-8421-0df61b574a48
  Boot ID:                    e4089dc4-96f2-4ecf-8df6-73d6b6f36d4a
  Kernel Version:             6.18.7-arch1-1
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                   ------------  ----------  ---------------  -------------  ---
  default                     kubernetes-demo-api-94ddfddc4-dq6wt    100m (0%)     500m (4%)   128Mi (0%)       512Mi (1%)     2m57s
  default                     kubernetes-demo-api-94ddfddc4-j9wkc    100m (0%)     500m (4%)   128Mi (0%)       512Mi (1%)     2m57s
  kube-system                 coredns-66bc5c9577-d6lcs               100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     6m22s
  kube-system                 etcd-minikube                          100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         6m28s
  kube-system                 kube-apiserver-minikube                250m (2%)     0 (0%)      0 (0%)           0 (0%)         6m28s
  kube-system                 kube-controller-manager-minikube       200m (1%)     0 (0%)      0 (0%)           0 (0%)         6m28s
  kube-system                 kube-proxy-96xwg                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         6m22s
  kube-system                 kube-scheduler-minikube                100m (0%)     0 (0%)      0 (0%)           0 (0%)         6m28s
  kube-system                 storage-provisioner                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6m27s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (7%)   1 (8%)
  memory             426Mi (1%)  1194Mi (3%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 6m21s                  kube-proxy       
  Normal  Starting                 6m32s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  6m32s (x8 over 6m32s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    6m32s (x8 over 6m32s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     6m32s (x7 over 6m32s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  6m32s                  kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 6m28s                  kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  6m28s                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  6m28s                  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    6m28s                  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     6m28s                  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeReady                6m28s                  kubelet          Node minikube status is now: NodeReady
  Normal  RegisteredNode           6m24s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.409144] [UFW BLOCK] IN=wlan0 OUT= MAC=33:33:00:01:00:03:e8:fb:1c:5c:49:29:86:dd SRC=fe80:0000:0000:0000:701a:2fcb:6378:064b DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=89 TC=0 HOPLIMIT=1 FLOWLBL=212228 PROTO=UDP SPT=61025 DPT=5355 LEN=49 
[  +0.000017] [UFW BLOCK] IN=wlan0 OUT= MAC=01:00:5e:00:00:fc:e8:fb:1c:5c:49:29:08:00 SRC=10.0.0.173 DST=224.0.0.252 LEN=69 TOS=0x00 PREC=0x00 TTL=1 ID=20925 PROTO=UDP SPT=61025 DPT=5355 LEN=49 
[Feb17 00:30] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:c807:68ff:fe5d:bad2 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=766103 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.008937] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=172.17.0.1 DST=224.0.0.252 LEN=53 TOS=0x00 PREC=0x00 TTL=255 ID=51448 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.177544] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=172.17.0.1 DST=224.0.0.252 LEN=53 TOS=0x00 PREC=0x00 TTL=255 ID=51513 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.139106] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:c807:68ff:fe5d:bad2 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=766103 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.000047] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=172.17.0.1 DST=224.0.0.252 LEN=53 TOS=0x00 PREC=0x00 TTL=255 ID=51526 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.249933] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:c807:68ff:fe5d:bad2 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=766103 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.000036] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=172.17.0.1 DST=224.0.0.252 LEN=53 TOS=0x00 PREC=0x00 TTL=255 ID=51604 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.249960] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:c807:68ff:fe5d:bad2 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=766103 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +1.629879] [UFW BLOCK] IN=br-10172edd586d OUT= PHYSIN=veth6ca56fd MAC=9e:19:68:5c:20:b4:da:40:cc:8a:eb:ea:08:00 SRC=192.168.49.2 DST=172.17.0.1 LEN=77 TOS=0x00 PREC=0x00 TTL=64 ID=7590 DF PROTO=UDP SPT=37842 DPT=53 LEN=57 
[  +0.120099] [UFW BLOCK] IN=br-10172edd586d OUT= MAC= SRC=192.168.49.1 DST=224.0.0.252 LEN=53 TOS=0x00 PREC=0x00 TTL=255 ID=36777 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.249986] [UFW BLOCK] IN=br-10172edd586d OUT= MAC= SRC=192.168.49.1 DST=224.0.0.252 LEN=53 TOS=0x00 PREC=0x00 TTL=255 ID=36858 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.249998] [UFW BLOCK] IN=br-10172edd586d OUT= MAC= SRC=192.168.49.1 DST=224.0.0.252 LEN=53 TOS=0x00 PREC=0x00 TTL=255 ID=37045 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +1.250094] [UFW BLOCK] IN=br-10172edd586d OUT= MAC= SRC=fe80:0000:0000:0000:9c19:68ff:fe5c:20b4 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=589088 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.000037] [UFW BLOCK] IN=veth6ca56fd OUT= MAC= SRC=fe80:0000:0000:0000:08b0:61ff:fe7e:faa4 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=285826 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.250128] [UFW BLOCK] IN=br-10172edd586d OUT= MAC= SRC=fe80:0000:0000:0000:9c19:68ff:fe5c:20b4 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=589088 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.000035] [UFW BLOCK] IN=veth6ca56fd OUT= MAC= SRC=fe80:0000:0000:0000:08b0:61ff:fe7e:faa4 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=285826 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.249700] [UFW BLOCK] IN=br-10172edd586d OUT= MAC= SRC=fe80:0000:0000:0000:9c19:68ff:fe5c:20b4 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=589088 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.000030] [UFW BLOCK] IN=veth6ca56fd OUT= MAC= SRC=fe80:0000:0000:0000:08b0:61ff:fe7e:faa4 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=285826 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +1.531798] [UFW BLOCK] IN=wlan0 OUT= MAC=01:00:5e:00:00:fc:e8:fb:1c:5c:49:29:08:00 SRC=10.0.0.173 DST=224.0.0.252 LEN=69 TOS=0x00 PREC=0x00 TTL=1 ID=20926 PROTO=UDP SPT=52670 DPT=5355 LEN=49 
[  +0.099252] [UFW BLOCK] IN=br-10172edd586d OUT= PHYSIN=veth6ca56fd MAC=9e:19:68:5c:20:b4:da:40:cc:8a:eb:ea:08:00 SRC=192.168.49.2 DST=172.17.0.1 LEN=77 TOS=0x00 PREC=0x00 TTL=64 ID=55122 DF PROTO=UDP SPT=58103 DPT=53 LEN=57 
[  +0.310256] [UFW BLOCK] IN=wlan0 OUT= MAC=01:00:5e:00:00:fc:e8:fb:1c:5c:49:29:08:00 SRC=10.0.0.173 DST=224.0.0.252 LEN=69 TOS=0x00 PREC=0x00 TTL=1 ID=20927 PROTO=UDP SPT=52670 DPT=5355 LEN=49 
[Feb17 00:32] [UFW BLOCK] IN=wlan0 OUT= MAC=33:33:00:01:00:03:e8:fb:1c:5c:49:29:86:dd SRC=fe80:0000:0000:0000:701a:2fcb:6378:064b DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=89 TC=0 HOPLIMIT=1 FLOWLBL=985770 PROTO=UDP SPT=52597 DPT=5355 LEN=49 
[  +0.000021] [UFW BLOCK] IN=wlan0 OUT= MAC=01:00:5e:00:00:fc:e8:fb:1c:5c:49:29:08:00 SRC=10.0.0.173 DST=224.0.0.252 LEN=69 TOS=0x00 PREC=0x00 TTL=1 ID=20928 PROTO=UDP SPT=52597 DPT=5355 LEN=49 
[  +0.511967] [UFW BLOCK] IN=wlan0 OUT= MAC=33:33:00:01:00:03:e8:fb:1c:5c:49:29:86:dd SRC=fe80:0000:0000:0000:701a:2fcb:6378:064b DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=89 TC=0 HOPLIMIT=1 FLOWLBL=985770 PROTO=UDP SPT=52597 DPT=5355 LEN=49 
[  +0.000020] [UFW BLOCK] IN=wlan0 OUT= MAC=01:00:5e:00:00:fc:e8:fb:1c:5c:49:29:08:00 SRC=10.0.0.173 DST=224.0.0.252 LEN=69 TOS=0x00 PREC=0x00 TTL=1 ID=20929 PROTO=UDP SPT=52597 DPT=5355 LEN=49 
[Feb17 00:33] [UFW BLOCK] IN=wlan0 OUT= MAC=01:00:5e:00:00:fc:e8:fb:1c:5c:49:29:08:00 SRC=10.0.0.173 DST=224.0.0.252 LEN=69 TOS=0x00 PREC=0x00 TTL=1 ID=20930 PROTO=UDP SPT=60308 DPT=5355 LEN=49 
[  +0.413127] [UFW BLOCK] IN=wlan0 OUT= MAC=33:33:00:01:00:03:e8:fb:1c:5c:49:29:86:dd SRC=fe80:0000:0000:0000:701a:2fcb:6378:064b DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=89 TC=0 HOPLIMIT=1 FLOWLBL=1029210 PROTO=UDP SPT=60308 DPT=5355 LEN=49 
[  +0.000030] [UFW BLOCK] IN=wlan0 OUT= MAC=01:00:5e:00:00:fc:e8:fb:1c:5c:49:29:08:00 SRC=10.0.0.173 DST=224.0.0.252 LEN=69 TOS=0x00 PREC=0x00 TTL=1 ID=20931 PROTO=UDP SPT=60308 DPT=5355 LEN=49 
[Feb17 00:38] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:c807:68ff:fe5d:bad2 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=766103 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.386763] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=172.17.0.1 DST=224.0.0.252 LEN=53 TOS=0x00 PREC=0x00 TTL=255 ID=40960 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.000045] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:c807:68ff:fe5d:bad2 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=766103 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.249927] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=172.17.0.1 DST=224.0.0.252 LEN=53 TOS=0x00 PREC=0x00 TTL=255 ID=41207 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.000051] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:c807:68ff:fe5d:bad2 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=766103 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.249951] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=172.17.0.1 DST=224.0.0.252 LEN=53 TOS=0x00 PREC=0x00 TTL=255 ID=41301 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.000045] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:c807:68ff:fe5d:bad2 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=766103 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +1.499947] [UFW BLOCK] IN=br-bfb8cdb5e3ae OUT= MAC= SRC=192.168.49.1 DST=224.0.0.252 LEN=53 TOS=0x00 PREC=0x00 TTL=255 ID=3226 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.249973] [UFW BLOCK] IN=br-bfb8cdb5e3ae OUT= MAC= SRC=192.168.49.1 DST=224.0.0.252 LEN=53 TOS=0x00 PREC=0x00 TTL=255 ID=3359 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.249985] [UFW BLOCK] IN=br-bfb8cdb5e3ae OUT= MAC= SRC=192.168.49.1 DST=224.0.0.252 LEN=53 TOS=0x00 PREC=0x00 TTL=255 ID=3383 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +1.250018] [UFW BLOCK] IN=vethfc48dc9 OUT= MAC= SRC=fe80:0000:0000:0000:985f:1dff:fef7:e1bc DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=914953 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.000031] [UFW BLOCK] IN=br-bfb8cdb5e3ae OUT= MAC= SRC=fe80:0000:0000:0000:60b2:d0ff:fe27:a355 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=816351 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.249946] [UFW BLOCK] IN=vethfc48dc9 OUT= MAC= SRC=fe80:0000:0000:0000:985f:1dff:fef7:e1bc DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=914953 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.000026] [UFW BLOCK] IN=br-bfb8cdb5e3ae OUT= MAC= SRC=fe80:0000:0000:0000:60b2:d0ff:fe27:a355 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=816351 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.249965] [UFW BLOCK] IN=vethfc48dc9 OUT= MAC= SRC=fe80:0000:0000:0000:985f:1dff:fef7:e1bc DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=914953 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.000022] [UFW BLOCK] IN=br-bfb8cdb5e3ae OUT= MAC= SRC=fe80:0000:0000:0000:60b2:d0ff:fe27:a355 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=816351 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[Feb17 00:39] [UFW BLOCK] IN=wlan0 OUT= MAC=33:33:00:01:00:03:e8:fb:1c:5c:49:29:86:dd SRC=fe80:0000:0000:0000:701a:2fcb:6378:064b DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=89 TC=0 HOPLIMIT=1 FLOWLBL=557734 PROTO=UDP SPT=58851 DPT=5355 LEN=49 
[  +0.000014] [UFW BLOCK] IN=wlan0 OUT= MAC=01:00:5e:00:00:fc:e8:fb:1c:5c:49:29:08:00 SRC=10.0.0.173 DST=224.0.0.252 LEN=69 TOS=0x00 PREC=0x00 TTL=1 ID=20932 PROTO=UDP SPT=58851 DPT=5355 LEN=49 
[  +0.410148] [UFW BLOCK] IN=wlan0 OUT= MAC=33:33:00:01:00:03:e8:fb:1c:5c:49:29:86:dd SRC=fe80:0000:0000:0000:701a:2fcb:6378:064b DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=89 TC=0 HOPLIMIT=1 FLOWLBL=557734 PROTO=UDP SPT=58851 DPT=5355 LEN=49 
[  +0.000017] [UFW BLOCK] IN=wlan0 OUT= MAC=01:00:5e:00:00:fc:e8:fb:1c:5c:49:29:08:00 SRC=10.0.0.173 DST=224.0.0.252 LEN=69 TOS=0x00 PREC=0x00 TTL=1 ID=20933 PROTO=UDP SPT=58851 DPT=5355 LEN=49 
[Feb17 00:40] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=172.17.0.1 DST=224.0.0.252 LEN=53 TOS=0x00 PREC=0x00 TTL=255 ID=50188 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[  +0.068217] [UFW BLOCK] IN=docker0 OUT= MAC= SRC=fe80:0000:0000:0000:c807:68ff:fe5d:bad2 DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=73 TC=0 HOPLIMIT=255 FLOWLBL=766103 PROTO=UDP SPT=5355 DPT=5355 LEN=33 
[Feb17 00:41] [UFW BLOCK] IN=wlan0 OUT= MAC=33:33:00:01:00:03:e8:fb:1c:5c:49:29:86:dd SRC=fe80:0000:0000:0000:701a:2fcb:6378:064b DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=89 TC=0 HOPLIMIT=1 FLOWLBL=632977 PROTO=UDP SPT=63658 DPT=5355 LEN=49 
[  +0.000014] [UFW BLOCK] IN=wlan0 OUT= MAC=01:00:5e:00:00:fc:e8:fb:1c:5c:49:29:08:00 SRC=10.0.0.173 DST=224.0.0.252 LEN=69 TOS=0x00 PREC=0x00 TTL=1 ID=20934 PROTO=UDP SPT=63658 DPT=5355 LEN=49 
[  +0.409968] [UFW BLOCK] IN=wlan0 OUT= MAC=33:33:00:01:00:03:e8:fb:1c:5c:49:29:86:dd SRC=fe80:0000:0000:0000:701a:2fcb:6378:064b DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=89 TC=0 HOPLIMIT=1 FLOWLBL=632977 PROTO=UDP SPT=63658 DPT=5355 LEN=49 
[  +0.000021] [UFW BLOCK] IN=wlan0 OUT= MAC=01:00:5e:00:00:fc:e8:fb:1c:5c:49:29:08:00 SRC=10.0.0.173 DST=224.0.0.252 LEN=69 TOS=0x00 PREC=0x00 TTL=1 ID=20935 PROTO=UDP SPT=63658 DPT=5355 LEN=49 
[Feb17 00:43] [UFW BLOCK] IN=wlan0 OUT= MAC=33:33:00:01:00:03:e8:fb:1c:5c:49:29:86:dd SRC=fe80:0000:0000:0000:701a:2fcb:6378:064b DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=89 TC=0 HOPLIMIT=1 FLOWLBL=183047 PROTO=UDP SPT=56495 DPT=5355 LEN=49 
[  +0.000017] [UFW BLOCK] IN=wlan0 OUT= MAC=01:00:5e:00:00:fc:e8:fb:1c:5c:49:29:08:00 SRC=10.0.0.173 DST=224.0.0.252 LEN=69 TOS=0x00 PREC=0x00 TTL=1 ID=20936 PROTO=UDP SPT=56495 DPT=5355 LEN=49 
[  +0.511964] [UFW BLOCK] IN=wlan0 OUT= MAC=33:33:00:01:00:03:e8:fb:1c:5c:49:29:86:dd SRC=fe80:0000:0000:0000:701a:2fcb:6378:064b DST=ff02:0000:0000:0000:0000:0000:0001:0003 LEN=89 TC=0 HOPLIMIT=1 FLOWLBL=183047 PROTO=UDP SPT=56495 DPT=5355 LEN=49 
[  +0.000016] [UFW BLOCK] IN=wlan0 OUT= MAC=01:00:5e:00:00:fc:e8:fb:1c:5c:49:29:08:00 SRC=10.0.0.173 DST=224.0.0.252 LEN=69 TOS=0x00 PREC=0x00 TTL=1 ID=20937 PROTO=UDP SPT=56495 DPT=5355 LEN=49 


==> etcd [96a99086fa2f] <==
{"level":"warn","ts":"2026-02-17T00:38:44.649628Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53430","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.653757Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53450","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.656439Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53476","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.660269Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53506","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.668006Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53530","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.670281Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53538","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.672997Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53570","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.675458Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53584","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.677940Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53600","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.680770Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53624","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.683136Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53646","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.685457Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53670","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.688371Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53688","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.690959Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53708","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.693868Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53712","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.696190Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53726","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.699053Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53748","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.701437Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53762","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.704114Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53788","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.706982Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53808","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.715518Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53822","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.718537Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53848","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.720913Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53850","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.723935Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53864","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.726716Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53880","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.729559Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53894","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.732064Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53910","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.734628Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53928","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.737346Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53940","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.739919Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53952","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.742571Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53970","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.745218Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53986","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.747782Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53998","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.750422Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54012","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.752951Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54034","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.756785Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54060","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.759112Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54068","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.761882Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54090","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.764360Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54118","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.767075Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54128","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.769928Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54154","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.772529Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54176","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.775033Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54190","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.777403Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54204","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.779988Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54226","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.786341Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54248","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.788541Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54252","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.791258Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54272","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.793969Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54286","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.796593Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54304","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.799302Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54322","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.801889Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54338","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.804477Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54352","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.807274Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54378","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.810033Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54390","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.812907Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54412","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.824541Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54446","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.827149Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54462","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.829993Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54486","server-name":"","error":"EOF"}
{"level":"warn","ts":"2026-02-17T00:38:44.845667Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54504","server-name":"","error":"EOF"}


==> kernel <==
 00:45:15 up  8:32,  0 users,  load average: 0.18, 0.20, 0.18
Linux minikube 6.18.7-arch1-1 #1 SMP PREEMPT_DYNAMIC Sat, 24 Jan 2026 00:47:39 +0000 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [8d8ef3bbbf95] <==
I0217 00:38:44.961541       1 shared_informer.go:349] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0217 00:38:44.961594       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0217 00:38:44.961601       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0217 00:38:44.961630       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0217 00:38:44.961424       1 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0217 00:38:44.961385       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
E0217 00:38:45.023145       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0217 00:38:45.049501       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I0217 00:38:45.051625       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0217 00:38:45.051633       1 policy_source.go:240] refreshing policies
I0217 00:38:45.061810       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I0217 00:38:45.061820       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I0217 00:38:45.061836       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I0217 00:38:45.061854       1 cache.go:39] Caches are synced for LocalAvailability controller
I0217 00:38:45.061868       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0217 00:38:45.061878       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0217 00:38:45.061868       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0217 00:38:45.061922       1 default_servicecidr_controller.go:166] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I0217 00:38:45.061868       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I0217 00:38:45.061983       1 aggregator.go:171] initial CRD sync complete...
I0217 00:38:45.061987       1 autoregister_controller.go:144] Starting autoregister controller
I0217 00:38:45.061989       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0217 00:38:45.061992       1 cache.go:39] Caches are synced for autoregister controller
I0217 00:38:45.062146       1 controller.go:667] quota admission added evaluator for: namespaces
I0217 00:38:45.065454       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0217 00:38:45.065458       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0217 00:38:45.065479       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0217 00:38:45.068571       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0217 00:38:45.068626       1 default_servicecidr_controller.go:228] Setting default ServiceCIDR condition Ready to True
I0217 00:38:45.074424       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0217 00:38:45.074587       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I0217 00:38:45.225004       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0217 00:38:45.965825       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0217 00:38:45.969390       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0217 00:38:45.969394       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0217 00:38:46.522623       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0217 00:38:46.551245       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0217 00:38:46.670239       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0217 00:38:46.677026       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0217 00:38:46.677529       1 controller.go:667] quota admission added evaluator for: endpoints
I0217 00:38:46.681217       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0217 00:38:46.971234       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0217 00:38:47.537198       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0217 00:38:47.552759       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0217 00:38:47.557554       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0217 00:38:52.875045       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0217 00:38:52.878643       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0217 00:38:52.972508       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0217 00:38:53.022524       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I0217 00:39:51.768572       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0217 00:40:11.520069       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0217 00:41:19.415591       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0217 00:41:32.267162       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0217 00:42:18.971150       1 alloc.go:328] "allocated clusterIPs" service="default/devops-kubernetes-api-service" clusterIPs={"IPv4":"10.106.45.95"}
I0217 00:42:19.494920       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0217 00:42:34.909681       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0217 00:43:37.112661       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0217 00:43:41.363085       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0217 00:44:43.459914       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0217 00:44:49.814880       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [6001d55ccb26] <==
I0217 00:38:51.873738       1 controllermanager.go:781] "Started controller" controller="ttl-after-finished-controller"
I0217 00:38:51.873744       1 controllermanager.go:759] "Warning: skipping controller" controller="storage-version-migrator-controller"
I0217 00:38:51.873843       1 ttlafterfinished_controller.go:112] "Starting TTL after finished controller" logger="ttl-after-finished-controller"
I0217 00:38:51.873848       1 shared_informer.go:349] "Waiting for caches to sync" controller="TTL after finished"
I0217 00:38:51.875868       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I0217 00:38:51.878769       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I0217 00:38:51.878838       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0217 00:38:51.883935       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I0217 00:38:51.891115       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0217 00:38:51.897228       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I0217 00:38:51.901375       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I0217 00:38:51.906507       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I0217 00:38:51.909753       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I0217 00:38:51.920876       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I0217 00:38:51.921953       1 shared_informer.go:356] "Caches are synced" controller="taint"
I0217 00:38:51.921972       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0217 00:38:51.922002       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0217 00:38:51.922008       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0217 00:38:51.922013       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0217 00:38:51.922019       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0217 00:38:51.922032       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0217 00:38:51.922111       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0217 00:38:51.924415       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I0217 00:38:51.924424       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I0217 00:38:51.924431       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I0217 00:38:51.924443       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I0217 00:38:51.924450       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0217 00:38:51.924459       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I0217 00:38:51.924462       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I0217 00:38:51.924478       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I0217 00:38:51.924535       1 shared_informer.go:356] "Caches are synced" controller="service account"
I0217 00:38:51.924711       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I0217 00:38:51.925584       1 shared_informer.go:356] "Caches are synced" controller="GC"
I0217 00:38:51.925605       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I0217 00:38:51.928422       1 shared_informer.go:356] "Caches are synced" controller="job"
I0217 00:38:51.971261       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I0217 00:38:51.971268       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0217 00:38:51.971272       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0217 00:38:51.971276       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0217 00:38:51.973258       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I0217 00:38:51.974379       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I0217 00:38:51.974392       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I0217 00:38:51.974408       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I0217 00:38:51.974412       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I0217 00:38:51.974410       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I0217 00:38:51.974421       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I0217 00:38:51.974434       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I0217 00:38:51.974423       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I0217 00:38:51.974522       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I0217 00:38:51.974635       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I0217 00:38:51.975173       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I0217 00:38:51.975176       1 shared_informer.go:356] "Caches are synced" controller="expand"
I0217 00:38:51.976572       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0217 00:38:51.978680       1 shared_informer.go:356] "Caches are synced" controller="node"
I0217 00:38:51.978692       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0217 00:38:51.978699       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0217 00:38:51.978703       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I0217 00:38:51.978706       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I0217 00:38:51.979826       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0217 00:38:51.983010       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]


==> kube-proxy [78d5126b03e9] <==
I0217 00:38:53.652573       1 server_linux.go:53] "Using iptables proxy"
I0217 00:38:53.695798       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I0217 00:38:53.795895       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I0217 00:38:53.795914       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E0217 00:38:53.795958       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0217 00:38:53.805961       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0217 00:38:53.805978       1 server_linux.go:132] "Using iptables Proxier"
I0217 00:38:53.808206       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0217 00:38:53.808595       1 server.go:527] "Version info" version="v1.34.0"
I0217 00:38:53.808608       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0217 00:38:53.809139       1 config.go:106] "Starting endpoint slice config controller"
I0217 00:38:53.809143       1 config.go:403] "Starting serviceCIDR config controller"
I0217 00:38:53.809147       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I0217 00:38:53.809149       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I0217 00:38:53.809160       1 config.go:200] "Starting service config controller"
I0217 00:38:53.809162       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I0217 00:38:53.809170       1 config.go:309] "Starting node config controller"
I0217 00:38:53.809173       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I0217 00:38:53.909499       1 shared_informer.go:356] "Caches are synced" controller="node config"
I0217 00:38:53.909500       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I0217 00:38:53.909499       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I0217 00:38:53.909514       1 shared_informer.go:356] "Caches are synced" controller="service config"


==> kube-scheduler [3ae466c294aa] <==
I0217 00:38:44.535906       1 serving.go:386] Generated self-signed cert in-memory
W0217 00:38:44.968158       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0217 00:38:44.968175       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0217 00:38:44.968181       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0217 00:38:44.968186       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0217 00:38:44.975395       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I0217 00:38:44.975404       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0217 00:38:44.976167       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0217 00:38:44.976179       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0217 00:38:44.976229       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0217 00:38:44.976366       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0217 00:38:44.976971       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0217 00:38:44.977026       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0217 00:38:44.977032       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0217 00:38:44.977035       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E0217 00:38:44.977054       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0217 00:38:44.977116       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0217 00:38:44.977211       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0217 00:38:44.977230       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0217 00:38:44.977230       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0217 00:38:44.977236       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0217 00:38:44.977271       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0217 00:38:44.977289       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0217 00:38:44.977317       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E0217 00:38:44.977331       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0217 00:38:44.977331       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0217 00:38:44.977338       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0217 00:38:44.977343       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0217 00:38:44.977351       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0217 00:38:44.977352       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E0217 00:38:45.791772       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0217 00:38:45.889375       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0217 00:38:45.894743       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E0217 00:38:45.904130       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0217 00:38:45.973524       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0217 00:38:46.006097       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0217 00:38:46.009387       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0217 00:38:46.042794       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0217 00:38:46.042837       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0217 00:38:46.138504       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0217 00:38:46.542545       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
I0217 00:38:48.576205       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Feb 17 00:42:47 minikube kubelet[2344]: W0217 00:42:47.425290    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:42:47 minikube kubelet[2344]: W0217 00:42:47.426830    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:42:47 minikube kubelet[2344]: W0217 00:42:47.426842    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:42:47 minikube kubelet[2344]: W0217 00:42:47.433761    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:42:57 minikube kubelet[2344]: W0217 00:42:57.435913    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:42:57 minikube kubelet[2344]: W0217 00:42:57.436106    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:42:57 minikube kubelet[2344]: W0217 00:42:57.436123    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:42:57 minikube kubelet[2344]: W0217 00:42:57.442586    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:07 minikube kubelet[2344]: W0217 00:43:07.444172    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:07 minikube kubelet[2344]: W0217 00:43:07.444373    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:07 minikube kubelet[2344]: W0217 00:43:07.444387    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:07 minikube kubelet[2344]: W0217 00:43:07.450468    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:17 minikube kubelet[2344]: W0217 00:43:17.452238    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:17 minikube kubelet[2344]: W0217 00:43:17.453575    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:17 minikube kubelet[2344]: W0217 00:43:17.453593    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:17 minikube kubelet[2344]: W0217 00:43:17.459751    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:27 minikube kubelet[2344]: W0217 00:43:27.461705    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:27 minikube kubelet[2344]: W0217 00:43:27.461863    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:27 minikube kubelet[2344]: W0217 00:43:27.461884    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:27 minikube kubelet[2344]: W0217 00:43:27.467745    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:37 minikube kubelet[2344]: W0217 00:43:37.469168    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:37 minikube kubelet[2344]: W0217 00:43:37.469423    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:37 minikube kubelet[2344]: W0217 00:43:37.469437    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:37 minikube kubelet[2344]: W0217 00:43:37.475629    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:47 minikube kubelet[2344]: W0217 00:43:47.477320    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:47 minikube kubelet[2344]: W0217 00:43:47.478684    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:47 minikube kubelet[2344]: W0217 00:43:47.478744    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:47 minikube kubelet[2344]: W0217 00:43:47.485211    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:57 minikube kubelet[2344]: W0217 00:43:57.486803    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:57 minikube kubelet[2344]: W0217 00:43:57.486992    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:57 minikube kubelet[2344]: W0217 00:43:57.487005    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:43:57 minikube kubelet[2344]: W0217 00:43:57.493525    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:07 minikube kubelet[2344]: W0217 00:44:07.495772    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:07 minikube kubelet[2344]: W0217 00:44:07.495962    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:07 minikube kubelet[2344]: W0217 00:44:07.495977    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:07 minikube kubelet[2344]: W0217 00:44:07.502219    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:17 minikube kubelet[2344]: W0217 00:44:17.503991    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:17 minikube kubelet[2344]: W0217 00:44:17.505289    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:17 minikube kubelet[2344]: W0217 00:44:17.505304    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:17 minikube kubelet[2344]: W0217 00:44:17.511502    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:27 minikube kubelet[2344]: W0217 00:44:27.513618    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:27 minikube kubelet[2344]: W0217 00:44:27.513823    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:27 minikube kubelet[2344]: W0217 00:44:27.513835    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:27 minikube kubelet[2344]: W0217 00:44:27.519541    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:37 minikube kubelet[2344]: W0217 00:44:37.521110    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:37 minikube kubelet[2344]: W0217 00:44:37.521307    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:37 minikube kubelet[2344]: W0217 00:44:37.521323    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:37 minikube kubelet[2344]: W0217 00:44:37.527598    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:47 minikube kubelet[2344]: W0217 00:44:47.529026    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:47 minikube kubelet[2344]: W0217 00:44:47.530474    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:47 minikube kubelet[2344]: W0217 00:44:47.530487    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:47 minikube kubelet[2344]: W0217 00:44:47.537485    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:57 minikube kubelet[2344]: W0217 00:44:57.539370    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:57 minikube kubelet[2344]: W0217 00:44:57.539558    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:57 minikube kubelet[2344]: W0217 00:44:57.539572    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:44:57 minikube kubelet[2344]: W0217 00:44:57.545772    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:45:07 minikube kubelet[2344]: W0217 00:45:07.547980    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:45:07 minikube kubelet[2344]: W0217 00:45:07.548179    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:45:07 minikube kubelet[2344]: W0217 00:45:07.548190    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory
Feb 17 00:45:07 minikube kubelet[2344]: W0217 00:45:07.554486    2344 fs.go:641] Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/root with error: no such file or directory


==> storage-provisioner [ff05d7a6cfeb] <==
W0217 00:44:15.503708       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:15.507339       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:17.509458       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:17.519353       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:19.520617       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:19.530705       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:21.531952       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:21.541750       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:23.543672       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:23.547639       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:25.548884       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:25.559082       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:27.560876       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:27.571042       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:29.572843       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:29.582919       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:31.584061       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:31.594658       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:33.596225       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:33.606405       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:35.608590       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:35.619077       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:37.620501       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:37.630324       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:39.631430       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:39.641473       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:41.642710       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:41.646448       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:43.647733       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:43.657875       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:45.659031       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:45.669557       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:47.671427       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:47.681496       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:49.682564       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:49.692640       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:51.693672       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:51.704189       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:53.705136       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:53.714836       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:55.716412       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:55.726426       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:57.728090       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:57.738316       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:59.739235       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:44:59.749395       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:45:01.750679       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:45:01.760926       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:45:03.762366       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:45:03.772235       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:45:05.773433       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:45:05.783209       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:45:07.784311       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:45:07.787976       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:45:09.789689       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:45:09.800197       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:45:11.801231       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:45:11.811143       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:45:13.812417       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0217 00:45:13.822120       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

